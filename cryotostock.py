"""
í†µí•© Streamlit ì•±: ì£¼ì‹ ë‰´ìŠ¤ + ì½”ì¸ ë‰´ìŠ¤
- ì‚¬ì´ë“œë°”ì—ì„œ 'ì£¼ì‹ ë‰´ìŠ¤' / 'ì½”ì¸ ë‰´ìŠ¤' ì„ íƒ í›„ ê°ê° ìˆ˜ì§‘Â·AI ìš”ì•½Â·ëª©ë¡ í‘œì‹œ
"""

import datetime
import os
import re
from email.utils import parsedate_to_datetime

import requests
import streamlit as st
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

st.set_page_config(
    page_title="ì£¼ì‹Â·ì½”ì¸ ë‰´ìŠ¤ ë¦¬í¬íŠ¸",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded",
)


def get_secret(key: str) -> str:
    try:
        return st.secrets.get(key, "") or os.getenv(key, "")
    except Exception:
        return os.getenv(key, "")


# â”€â”€ API í‚¤ (ê³µí†µ + ì£¼ì‹/ì½”ì¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FINNHUB_API_KEY = get_secret("FINNHUB_API_KEY")
CRYPTOPANIC_API_KEY = get_secret("CRYPTOPANIC_API_KEY")
OPENAI_API_KEY = get_secret("OPENAI_API_KEY")
GEMINI_API_KEY = get_secret("GEMINI_API_KEY")
APP_PASSWORD = get_secret("APP_PASSWORD")

# â”€â”€ ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ (í•œ ë²ˆë§Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if APP_PASSWORD and not st.session_state.authenticated:
    st.markdown("""
    <style>
    #MainMenu, header, footer { visibility: hidden; }
    .lock-wrap {
        max-width: 380px; margin: 10vh auto 0;
        background: #161b22; border: 1px solid #30363d;
        border-radius: 20px; padding: 48px 40px;
        text-align: center; box-shadow: 0 24px 64px rgba(0,0,0,.6);
    }
    .lock-wrap h2 { font-size: 1.4rem; font-weight: 700; color: #f0f6fc; margin-bottom: 6px; }
    .lock-wrap p { color: #8b949e; font-size: .88rem; margin-bottom: 28px; }
    </style>
    """, unsafe_allow_html=True)
    st.markdown(
        '<div class="lock-wrap"><div style="font-size:2.6rem">ğŸ”</div><h2>ì£¼ì‹Â·ì½”ì¸ ë‰´ìŠ¤ ë¦¬í¬íŠ¸</h2><p>ì ‘ê·¼í•˜ë ¤ë©´ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”</p></div>',
        unsafe_allow_html=True,
    )
    pw = st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", placeholder="â€¢â€¢â€¢â€¢", label_visibility="collapsed")
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("ì ê¸ˆ í•´ì œ", type="primary", use_container_width=True):
            if pw == APP_PASSWORD:
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
    st.stop()

# â”€â”€ ê³µí†µ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NOW_UTC = datetime.datetime.utcnow()
NOW_KST = NOW_UTC + datetime.timedelta(hours=9)
TODAY_STR = NOW_KST.strftime("%Y-%m-%d")
YESTERDAY_STR = (NOW_KST - datetime.timedelta(days=1)).strftime("%Y-%m-%d")

SOURCE_COLORS = {
    # ì£¼ì‹
    "finnhub": "#2196F3",
    "yahoo finance": "#720e9e",
    "cnbc": "#005594",
    "marketwatch": "#40a829",
    "reuters": "#ff8000",
    "mni markets": "#e63946",
    "mkt news": "#f4a261",
    # ì½”ì¸
    "cryptopanic": "#f7931a",
    "coindesk": "#1a73e8",
    "cryptonews.net": "#2ea043",
    "coincarp": "#8b5cf6",
    "crypto.news": "#06b6d4",
    "cryptonews.com": "#ef4444",
    "the block": "#f59e0b",
    "decrypt": "#00d4aa",
}

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9",
}

# â”€â”€ ê³µí†µ CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("""
<style>
.main-header { background: linear-gradient(135deg, #0a192f 0%, #112240 50%, #1a1f2e 100%); border: 1px solid #233554; border-radius: 16px; padding: 32px 28px 24px; text-align: center; margin-bottom: 24px; }
.main-header h1 { font-size: 2rem; font-weight: 800; background: linear-gradient(90deg, #64ffda, #2196F3, #f7931a); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; margin-bottom: 8px; }
.main-header .sub { color: #8892b0; font-size: .9rem; }
.news-card { background: #161b22; border: 1px solid #21262d; border-radius: 10px; padding: 14px 16px; margin-bottom: 8px; }
.news-card:hover { border-color: #64ffda; }
.news-title { font-size: .93rem; font-weight: 500; color: #e6edf3; line-height: 1.5; margin-bottom: 5px; }
.news-title a { color: #e6edf3; text-decoration: none; }
.news-title a:hover { color: #58a6ff; }
.news-desc { font-size: .8rem; color: #8b949e; line-height: 1.5; margin-bottom: 7px; }
.news-meta { display: flex; flex-wrap: wrap; gap: 5px; align-items: center; }
.src-badge { font-size: .7rem; border: 1px solid; border-radius: 4px; padding: 1px 7px; font-weight: 600; }
.time-tag { font-size: .72rem; color: #6e7681; }
.sec-title { font-size: 1rem; font-weight: 700; color: #f0f6fc; margin: 24px 0 12px; padding-left: 10px; border-left: 4px solid #64ffda; }
</style>
""", unsafe_allow_html=True)


# â”€â”€ ê³µí†µ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def src_color(source: str) -> str:
    low = source.lower()
    for k, v in SOURCE_COLORS.items():
        if k in low:
            return v
    return "#8b949e"


def _strip_html(text: str) -> str:
    if not text:
        return ""
    cleaned = BeautifulSoup(text, "html.parser").get_text(separator=" ")
    return re.sub(r"\s+", " ", cleaned).strip()


def make_item(title, url="", source="", published_at="", description=""):
    desc = _strip_html(description or "")
    return {
        "title": re.sub(r"\s+", " ", title).strip(),
        "url": url,
        "source": source,
        "published_at": published_at,
        "description": desc,
    }


def is_recent(pub: str) -> bool:
    if not pub:
        return True
    return pub[:10] >= YESTERDAY_STR


def dedup(news_list: list) -> list:
    seen, result = {}, []
    for item in news_list:
        key = re.sub(r"[^a-z0-9]", "", item["title"].lower())[:60]
        if key not in seen:
            seen[key] = True
            result.append(item)
    return result


def utc_to_kst(iso_str: str) -> str:
    if not iso_str:
        return ""
    try:
        dt = datetime.datetime.fromisoformat(iso_str.replace("Z", "+00:00"))
        return (dt + datetime.timedelta(hours=9)).strftime("%m/%d %H:%M")
    except Exception:
        return iso_str[:16]


def build_news_text(news_list: list, limit: int = 60) -> str:
    lines = []
    for item in news_list[:limit]:
        line = f"- [{item['source']}] {item['title']}"
        if item.get("description"):
            line += f"\n  {item['description'][:120]}"
        lines.append(line)
    return "\n".join(lines)


def render_news_card(item: dict, idx: int) -> None:
    import html as _html

    title = _html.escape(item.get("title", "") or "")
    url = item.get("url", "")
    source = item.get("source", "")
    pub = item.get("published_at", "")
    desc = _html.escape((item.get("description", "") or "").strip())
    color = src_color(source)
    kst = utc_to_kst(pub)

    title_html = f'<a href="{url}" target="_blank">{title}</a>' if url else title
    desc_html = f'<div class="news-desc">{desc[:150]}</div>' if desc and desc != title else ""
    time_html = f'<span class="time-tag">ğŸ• KST {kst}</span>' if kst else ""

    st.markdown(f"""
    <div class="news-card">
      <div style="display:flex;gap:10px;align-items:flex-start">
        <div style="flex-shrink:0;width:24px;height:24px;background:#21262d;border-radius:5px; display:flex;align-items:center;justify-content:center; font-size:.7rem;color:#8b949e;font-weight:600;margin-top:2px">{idx}</div>
        <div style="flex:1;min-width:0">
          <div class="news-title">{title_html}</div>
          {desc_html}
          <div class="news-meta">
            <span class="src-badge" style="background:{color}22;color:{color};border-color:{color}55">{source}</span>
            {time_html}
          </div>
        </div>
      </div>
    </div>
    """, unsafe_allow_html=True)


# â”€â”€ AI ìš”ì•½ (í”„ë¡¬í”„íŠ¸ ì¸ìë¡œ ì£¼ì‹/ì½”ì¸ êµ¬ë¶„) â”€â”€â”€â”€â”€â”€
def summarize_gemini(news_list: list, api_key: str, prompt_quick: str, prompt_deep: str) -> tuple:
    try:
        from google import genai
        from google.genai import types
    except ImportError:
        st.error("google-genai íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return "", ""

    client = genai.Client(api_key=api_key)
    content = build_news_text(news_list, 60)

    def _extract(resp):
        if resp.text is not None:
            return resp.text
        try:
            return resp.candidates[0].content.parts[0].text or ""
        except Exception:
            return ""

    quick, deep = "", ""
    try:
        resp = client.models.generate_content(
            model="gemini-2.5-pro",
            contents=prompt_quick.format(date=TODAY_STR, content=content),
            config=types.GenerateContentConfig(temperature=0.4, max_output_tokens=8000),
        )
        quick = _extract(resp)
    except Exception as e:
        st.warning(f"Gemini Quick Summary ì˜¤ë¥˜: {e}")
    try:
        resp = client.models.generate_content(
            model="gemini-2.5-pro",
            contents=prompt_deep.format(date=TODAY_STR, content=content),
            config=types.GenerateContentConfig(temperature=0.35, max_output_tokens=16000),
        )
        deep = _extract(resp)
    except Exception as e:
        st.warning(f"Gemini Deep Dive ì˜¤ë¥˜: {e}")
    return quick, deep


def summarize_openai(news_list: list, api_key: str, prompt_quick: str, prompt_deep: str) -> tuple:
    try:
        from openai import OpenAI
    except ImportError:
        st.error("openai íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return "", ""

    client = OpenAI(api_key=api_key)
    content = build_news_text(news_list, 60)
    quick, deep = "", ""
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt_quick.format(date=TODAY_STR, content=content)}],
            max_tokens=1200,
            temperature=0.4,
        )
        quick = resp.choices[0].message.content or ""
    except Exception as e:
        st.warning(f"GPT Quick Summary ì˜¤ë¥˜: {e}")
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt_deep.format(date=TODAY_STR, content=content)}],
            max_tokens=2500,
            temperature=0.35,
        )
        deep = resp.choices[0].message.content or ""
    except Exception as e:
        st.warning(f"GPT Deep Dive ì˜¤ë¥˜: {e}")
    return quick, deep


# â”€â”€ ì£¼ì‹ ì „ìš©: ìŠ¤í¬ë˜í¼ + í”„ë¡¬í”„íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT_STOCK_QUICK = """ë‹¤ìŒì€ {date} (KST) ë¯¸êµ­ ì£¼ì‹ ë° ê¸ˆìœµ ì‹œì¥ ë‰´ìŠ¤ì…ë‹ˆë‹¤.

{content}

ìœ„ ë‰´ìŠ¤ë§Œì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ Quick Summaryë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
1. **ì˜¤ëŠ˜ì˜ ì¦ì‹œ í•µì‹¬ í…Œë§ˆ** (ê±°ì‹œê²½ì œ, S&P500 íë¦„ ë“± 3~5ê°€ì§€, ê° 1~2ë¬¸ì¥)
2. **ì£¼ìš” ê¸°ì—…/ì„¹í„°ë³„ ì´ìŠˆ** (íŠ¹ì§•ì£¼ ì¤‘ì‹¬, ê° 1ë¬¸ì¥)
3. **í•œì¤„ ì‹œì¥ ìš”ì•½** (ì „ì²´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ)
ê°€ë…ì„± ì¢‹ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

PROMPT_STOCK_DEEP = """ë‹¤ìŒì€ {date} (KST) ë¯¸êµ­ ì¦ì‹œ ì£¼ìš” ë‰´ìŠ¤ì…ë‹ˆë‹¤.

{content}

ìœ„ ë‰´ìŠ¤ë§Œì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ Deep Dive ì‹¬ì¸µ ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
1. **ê±°ì‹œ ê²½ì œ ë° ì—°ì¤€(Fed) ë™í–¥ ë¶„ì„** (ê¸ˆë¦¬, ì¸í”Œë ˆì´ì…˜ ë“±)
2. **ì£¼ìš” ê¸°ì—… ì‹¤ì  ë° í€ë”ë©˜í„¸ ë¶„ì„** (ì–¸ê¸‰ëœ ê¸°ì—… ìœ„ì£¼ ìƒì„¸íˆ)
3. **ì„¹í„°ë³„ ìê¸ˆ íë¦„ ë° íŠ¹ì§•** (ê¸°ìˆ ì£¼, ê¸ˆìœµì£¼ ë“±)
4. **ë¦¬ìŠ¤í¬ ìš”ì¸ ë° ì‹œì¥ì˜ ìš°ë ¤**
5. **ë‹¨ê¸° ì‹œì¥ ì „ë§ ë° ì›”ê°€ ì‹œê°**
ê° ì„¹ì…˜ì„ ì „ë¬¸ì ì¸ ê¸ˆìœµ ë¦¬í¬íŠ¸ í†¤ìœ¼ë¡œ ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""


def fetch_finnhub(api_key: str) -> list:
    if not api_key:
        return []
    try:
        r = requests.get(
            "https://finnhub.io/api/v1/news",
            params={"category": "general", "token": api_key},
            headers=HEADERS,
            timeout=15,
        )
        r.raise_for_status()
        data = r.json()
    except Exception:
        return []
    results = []
    for item in data[:30]:
        try:
            dt = datetime.datetime.utcfromtimestamp(item.get("datetime", 0))
            pub = dt.strftime("%Y-%m-%dT%H:%M:%SZ")
            if not is_recent(pub):
                continue
            results.append(
                make_item(
                    title=item.get("headline", ""),
                    url=item.get("url", ""),
                    source=item.get("source", "Finnhub"),
                    published_at=pub,
                    description=item.get("summary", ""),
                )
            )
        except Exception:
            continue
    return results


def fetch_mktnews() -> list:
    results = []
    try:
        import time as _time
        t = int(_time.time() * 1000)
        r = requests.get(
            f"https://static.mktnews.net/json/flash/en.json?t={t}",
            headers=HEADERS,
            timeout=15,
        )
        if r.status_code != 200:
            return []
        data = r.json()
        for item in data[:50]:
            try:
                content = (item.get("data") or {}).get("content", "").strip()
                title_field = (item.get("data") or {}).get("title", "").strip()
                title = title_field if title_field else content[:120]
                if not title:
                    continue
                pub = item.get("time", "")
                if pub and not is_recent(pub):
                    continue
                item_id = item.get("id", "")
                url = f"https://mktnews.com/flashDetail.html?id={item_id}" if item_id else ""
                desc = content if title_field and content != title else ""
                results.append(
                    make_item(title=title, url=url, source="MKT News", published_at=pub, description=desc)
                )
            except Exception:
                continue
    except Exception:
        pass
    return results


def fetch_mni_markets() -> list:
    results = []
    try:
        r = requests.get("https://www.mnimarkets.com/articles", headers=HEADERS, timeout=15)
        if r.status_code != 200:
            r = requests.get("https://www.mnimarkets.com/", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(r.text, "html.parser")
        seen_urls = set()
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if "/articles/" not in href:
                continue
            url = href if href.startswith("http") else "https://www.mnimarkets.com" + href
            if url in seen_urls:
                continue
            seen_urls.add(url)
            title = a.get_text(strip=True)
            if not title or len(title) < 10:
                parent = a.find_parent()
                if parent:
                    title = parent.get_text(separator=" ", strip=True)[:200]
            if not title or len(title) < 10:
                continue
            results.append(make_item(title=title[:200], url=url, source="MNI Markets", published_at="", description=""))
            if len(results) >= 30:
                break
    except Exception:
        pass
    return results


def fetch_rss_feed(rss_url: str, source_name: str) -> list:
    results = []
    try:
        r = requests.get(rss_url, headers=HEADERS, timeout=15)
        if r.status_code != 200:
            return []
        soup = BeautifulSoup(r.text, "xml")
        for item in soup.find_all("item") or soup.find_all("entry"):
            title_el = item.find("title")
            link_el = item.find("link")
            pub_el = item.find("pubDate") or item.find("published")
            desc_el = item.find("description") or item.find("summary")
            title = title_el.get_text(strip=True) if title_el else ""
            link = link_el.get_text(strip=True) if link_el else ""
            pub_raw = pub_el.get_text(strip=True) if pub_el else ""
            desc = (
                BeautifulSoup(desc_el.get_text(strip=True), "html.parser").get_text(strip=True)[:200]
                if desc_el
                else ""
            )
            if not title:
                continue
            try:
                pub_iso = parsedate_to_datetime(pub_raw).strftime("%Y-%m-%dT%H:%M:%SZ")
            except Exception:
                pub_iso = pub_raw[:19]
            if pub_iso and not is_recent(pub_iso):
                continue
            results.append(make_item(title=title, url=link, source=source_name, published_at=pub_iso, description=desc))
    except Exception:
        pass
    return results


# â”€â”€ ì½”ì¸ ì „ìš©: ìœ í‹¸ + ìŠ¤í¬ë˜í¼ + í”„ë¡¬í”„íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€
PROMPT_COIN_QUICK = """ë‹¤ìŒì€ {date} (KST) ê¸°ì¤€ ì½”ì¸ ë‰´ìŠ¤ì…ë‹ˆë‹¤.

{content}

ìœ„ ë‰´ìŠ¤ë§Œ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ Quick Summaryë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
1. **ì˜¤ëŠ˜ì˜ í•µì‹¬ ì´ìŠˆ** (3~5ê°œ, ê° 1~2ë¬¸ì¥)
2. **ì½”ì¸/í”„ë¡œì íŠ¸ë³„ ì£¼ìš” ì´ìŠˆ** (ì–¸ê¸‰ëœ ì½”ì¸ ì¤‘ì‹¬, ê° 1ë¬¸ì¥)
3. **ì‹œì¥ í•œì¤„ ìš”ì•½** (ì „ì²´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ)
ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

PROMPT_COIN_DEEP = """ë‹¤ìŒì€ {date} (KST) ê¸°ì¤€ ì½”ì¸ ë‰´ìŠ¤ì…ë‹ˆë‹¤.

{content}

ìœ„ ë‰´ìŠ¤ë§Œ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ Deep Dive ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
1. **ê±°ì‹œ ê²½ì œ ë° ê·œì œ í™˜ê²½ ë¶„ì„**
2. **ì£¼ìš” ì½”ì¸ë³„/ì„¹í„°ë³„ í…Œë§ˆ ë¶„ì„** (ê° ì½”ì¸ 2~4ë¬¸ì¥)
3. **ê¸°ê´€ íˆ¬ìì ë™í–¥** (ETF, ê¸°ì—… ë³´ìœ , ê¸°ê´€ í¬ì§€ì…˜)
4. **ë¦¬ìŠ¤í¬ ìš”ì¸ ë° ì£¼ì˜ í¬ì¸íŠ¸**
5. **ë‹¨ê¸° ì‹œì¥ ì „ë§ ë° íˆ¬ì ì‹œì‚¬ì **
ê° ì„¹ì…˜ì„ ì¶©ë¶„íˆ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""


def find_time_in_parents(element):
    current = element
    for _ in range(6):
        if not current:
            break
        current = getattr(current, "parent", None)
        if not current:
            break
        time_tag = current.find("time")
        if time_tag:
            return time_tag.get("datetime", "")
    return ""


def parse_rss_datetime(raw: str) -> str:
    try:
        return parsedate_to_datetime(raw).strftime("%Y-%m-%dT%H:%M:%SZ")
    except Exception:
        return raw[:19]


def fetch_cryptopanic(api_key: str) -> list:
    if not api_key:
        return []
    try:
        response = requests.get(
            "https://cryptopanic.com/api/developer/v2/posts/",
            params={"auth_token": api_key, "public": "true", "kind": "news", "regions": "en"},
            headers=HEADERS,
            timeout=15,
        )
        if response.status_code in (403, 429):
            return []
        response.raise_for_status()
        data = response.json()
    except Exception:
        return []
    results = []
    for item in data.get("results", []):
        pub = item.get("published_at", "")
        if not is_recent(pub):
            continue
        results.append(
            make_item(
                title=item.get("title", ""),
                source="CryptoPanic",
                published_at=pub,
                description=item.get("description", "") or "",
            )
        )
    return results


def fetch_coindesk() -> list:
    try:
        response = requests.get("https://www.coindesk.com/latest-crypto-news", headers=HEADERS, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
    except Exception:
        return []
    results = []
    seen = set()
    for selector in ["a[href*='/markets/']", "a[href*='/business/']", "a[href*='/tech/']", "a[href*='/policy/']"]:
        for link in soup.select(selector):
            href = link.get("href", "")
            title = link.get_text(strip=True)
            if not title or len(title) < 15 or href in seen:
                continue
            seen.add(href)
            full_url = f"https://www.coindesk.com{href}" if href.startswith("/") else href
            pub = find_time_in_parents(link)
            if pub and not is_recent(pub):
                continue
            results.append(make_item(title=title, url=full_url, source="CoinDesk", published_at=pub))
    return results


def fetch_cryptonews_net() -> list:
    results = []
    seen = set()
    for url in [
        "https://cryptonews.net/news/bitcoin/",
        "https://cryptonews.net/news/ethereum/",
        "https://cryptonews.net/",
    ]:
        try:
            response = requests.get(url, headers=HEADERS, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
        except Exception:
            continue
        for item in soup.select(".news-item"):
            link = item.find("a", href=True)
            if not link:
                continue
            href = link["href"]
            full_url = f"https://cryptonews.net{href}" if href.startswith("/") else href
            if full_url in seen:
                continue
            seen.add(full_url)
            title_el = item.select_one(".news-item__title, h2, h3, h4, .title")
            title = title_el.get_text(strip=True) if title_el else item.get_text(separator=" ", strip=True)[:120]
            time_el = item.find("time")
            pub = time_el.get("datetime", "") if time_el else ""
            if pub and not is_recent(pub):
                continue
            source_el = item.select_one(".news-item__source, .source")
            source = source_el.get_text(strip=True) if source_el else "cryptonews.net"
            results.append(make_item(title=title, url=full_url, source=source or "cryptonews.net", published_at=pub))
    return results


def fetch_coincarp() -> list:
    results = []
    seen = set()
    for url in [
        "https://www.coincarp.com/news/bitcoin/",
        "https://www.coincarp.com/news/ethereum/",
        "https://www.coincarp.com/news/",
    ]:
        try:
            response = requests.get(url, headers=HEADERS, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
        except Exception:
            continue
        for link in soup.find_all("a", href=True):
            href = link.get("href", "")
            if not href.startswith("http") or "coincarp.com" in href:
                continue
            raw = link.get_text(strip=True)
            title = re.sub(r"^\d+\s*(min|mins|hour|hours|sec|secs|day|days)\s*(Ago|ago)\s*", "", raw).strip()
            if not title or len(title) < 15 or href in seen:
                continue
            seen.add(href)
            match = re.search(r"(\d+)\s*(min|mins|hour|hours)", raw)
            pub = ""
            if match:
                value = int(match.group(1))
                delta = datetime.timedelta(minutes=value) if "min" in match.group(2) else datetime.timedelta(hours=value)
                pub = (NOW_UTC - delta).strftime("%Y-%m-%dT%H:%M:%SZ")
            domain = re.search(r"https?://(?:www\.)?([^/]+)", href)
            source = domain.group(1) if domain else "coincarp"
            results.append(make_item(title=title, url=href, source=source, published_at=pub))
    return results


def fetch_theblock_rss() -> list:
    results = []
    for rss_url in ["https://www.theblock.co/rss.xml", "https://www.theblock.co/feeds/rss.xml"]:
        try:
            response = requests.get(rss_url, headers=HEADERS, timeout=15)
            if response.status_code != 200:
                continue
            soup = BeautifulSoup(response.text, "xml")
        except Exception:
            continue
        for item in soup.find_all("item") or soup.find_all("entry"):
            title_el = item.find("title")
            link_el = item.find("link")
            pub_el = item.find("pubDate") or item.find("published") or item.find("updated")
            desc_el = item.find("description") or item.find("summary")
            title = title_el.get_text(strip=True) if title_el else ""
            link = link_el.get_text(strip=True) if link_el else ""
            pub_raw = pub_el.get_text(strip=True) if pub_el else ""
            desc = (
                BeautifulSoup(desc_el.get_text(strip=True), "html.parser").get_text(strip=True)[:200] if desc_el else ""
            )
            if not title:
                continue
            pub_iso = parse_rss_datetime(pub_raw)
            if pub_iso and not is_recent(pub_iso):
                continue
            results.append(make_item(title=title, url=link, source="The Block", published_at=pub_iso, description=desc))
        if results:
            break
    return results


def fetch_cryptonews_com() -> list:
    results = []
    seen = set()
    for url in [
        "https://cryptonews.com/news/",
        "https://cryptonews.com/news/bitcoin-news/",
        "https://cryptonews.com/news/ethereum-news/",
    ]:
        try:
            response = requests.get(url, headers=HEADERS, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
        except Exception:
            continue
        for link in soup.find_all("a", href=True):
            href = link.get("href", "")
            full_url = f"https://cryptonews.com{href}" if href.startswith("/") else href
            if not re.search(r"cryptonews\.com/news/[a-z]", full_url):
                continue
            title = link.get_text(strip=True)
            if not title or len(title) < 15 or full_url in seen:
                continue
            seen.add(full_url)
            pub = find_time_in_parents(link)
            if pub and not is_recent(pub):
                continue
            results.append(make_item(title=title, url=full_url, source="cryptonews.com", published_at=pub))
    return results


def fetch_decrypt() -> list:
    try:
        response = requests.get("https://decrypt.co/feed", headers=HEADERS, timeout=15)
        if response.status_code != 200:
            return []
        soup = BeautifulSoup(response.text, "xml")
    except Exception:
        return []
    results = []
    for item in soup.find_all("item"):
        title_el = item.find("title")
        link_el = item.find("link")
        pub_el = item.find("pubDate")
        desc_el = item.find("description")
        title = title_el.get_text(strip=True) if title_el else ""
        link = link_el.get_text(strip=True) if link_el else ""
        pub_raw = pub_el.get_text(strip=True) if pub_el else ""
        desc = BeautifulSoup(desc_el.get_text(strip=True), "html.parser").get_text(strip=True)[:200] if desc_el else ""
        if not title:
            continue
        pub_iso = parse_rss_datetime(pub_raw)
        if pub_iso and not is_recent(pub_iso):
            continue
        results.append(make_item(title=title, url=link, source="Decrypt", published_at=pub_iso, description=desc))
    return results


# â”€â”€ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì£¼ì‹/ì½”ì¸ ë¶„ë¦¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_session():
    for prefix in ("stock_", "coin_"):
        if f"{prefix}news_data" not in st.session_state:
            st.session_state[f"{prefix}news_data"] = []
            st.session_state[f"{prefix}source_stats"] = {}
            st.session_state[f"{prefix}summary_quick"] = ""
            st.session_state[f"{prefix}summary_deep"] = ""
            st.session_state[f"{prefix}provider"] = ""


init_session()

# â”€â”€ ì‚¬ì´ë“œë°”: ëª¨ë“œ ì„ íƒ + ëª¨ë“œë³„ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.markdown("### ğŸ“Š ë©”ë‰´")
    mode = st.radio("ì„ íƒ", ["ğŸ“ˆ ì£¼ì‹ ë‰´ìŠ¤", "ğŸª™ ì½”ì¸ ë‰´ìŠ¤"], label_visibility="collapsed")
    is_stock = mode == "ğŸ“ˆ ì£¼ì‹ ë‰´ìŠ¤"
    st.markdown("---")
    st.markdown("### âš™ï¸ ì„¤ì •")
    use_ai = st.toggle("AI ìš”ì•½ ìƒì„±", value=bool(GEMINI_API_KEY or OPENAI_API_KEY))
    if use_ai:
        _opts = []
        if GEMINI_API_KEY:
            _opts.append("Gemini 2.5 Pro")
        if OPENAI_API_KEY:
            _opts.append("GPT-4o-mini")
        if not _opts:
            _opts = ["(API í‚¤ ì—†ìŒ)"]
        ai_provider = st.selectbox("AI ì œê³µì", _opts)
    else:
        ai_provider = ""
    st.markdown("---")
    st.markdown("**ìˆ˜ì§‘ ì†ŒìŠ¤**")

    if is_stock:
        src_finnhub = st.checkbox("Finnhub API", value=bool(FINNHUB_API_KEY))
        src_yahoo = st.checkbox("Yahoo Finance (RSS)", value=True)
        src_cnbc = st.checkbox("CNBC (RSS)", value=True)
        src_marketwatch = st.checkbox("MarketWatch (RSS)", value=True)
        src_mni = st.checkbox("MNI Markets (ìŠ¤í¬ë˜í•‘)", value=True)
        src_mktnews = st.checkbox("MKT News (API)", value=True)
        run_btn = st.button("ğŸš€ ì£¼ì‹ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘", type="primary", use_container_width=True)
    else:
        src_cryptopanic = st.checkbox("CryptoPanic API", value=bool(CRYPTOPANIC_API_KEY))
        src_coindesk = st.checkbox("CoinDesk", value=True)
        src_cryptonews_n = st.checkbox("cryptonews.net", value=True)
        src_coincarp = st.checkbox("coincarp.com", value=True)
        src_theblock = st.checkbox("The Block (RSS)", value=True)
        src_cryptonews_c = st.checkbox("cryptonews.com", value=True)
        src_decrypt = st.checkbox("Decrypt (RSS)", value=True)
        run_btn = st.button("ğŸš€ ì½”ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘", type="primary", use_container_width=True)

    st.markdown("---")
    st.caption(f"KST {NOW_KST.strftime('%Y-%m-%d %H:%M')}")


# â”€â”€ ìˆ˜ì§‘ ì‹¤í–‰ (ëª¨ë“œë³„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if run_btn:
    all_news = []
    source_map = {}

    if is_stock:
        tasks = []
        if src_finnhub and FINNHUB_API_KEY:
            tasks.append(("Finnhub API", fetch_finnhub, [FINNHUB_API_KEY]))
        if src_yahoo:
            tasks.append(("Yahoo Finance", fetch_rss_feed, ["https://finance.yahoo.com/news/rssindex", "Yahoo Finance"]))
        if src_cnbc:
            tasks.append(("CNBC", fetch_rss_feed, ["https://search.cnbc.com/rs/search/combinedcms/view.xml?profile=120000000", "CNBC"]))
        if src_marketwatch:
            tasks.append(("MarketWatch", fetch_rss_feed, ["http://feeds.marketwatch.com/marketwatch/topstories/", "MarketWatch"]))
        if src_mni:
            tasks.append(("MNI Markets", fetch_mni_markets, []))
        if src_mktnews:
            tasks.append(("MKT News", fetch_mktnews, []))

        prompt_quick, prompt_deep = PROMPT_STOCK_QUICK, PROMPT_STOCK_DEEP
        prefix = "stock_"
    else:
        tasks = []
        if src_cryptopanic and CRYPTOPANIC_API_KEY:
            tasks.append(("CryptoPanic", fetch_cryptopanic, [CRYPTOPANIC_API_KEY]))
        if src_coindesk:
            tasks.append(("CoinDesk", fetch_coindesk, []))
        if src_cryptonews_n:
            tasks.append(("cryptonews.net", fetch_cryptonews_net, []))
        if src_coincarp:
            tasks.append(("coincarp.com", fetch_coincarp, []))
        if src_theblock:
            tasks.append(("The Block", fetch_theblock_rss, []))
        if src_cryptonews_c:
            tasks.append(("cryptonews.com", fetch_cryptonews_com, []))
        if src_decrypt:
            tasks.append(("Decrypt", fetch_decrypt, []))
        prompt_quick, prompt_deep = PROMPT_COIN_QUICK, PROMPT_COIN_DEEP
        prefix = "coin_"

    with st.status("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...", expanded=True) as status:
        for name, fn, args in tasks:
            st.write(f"ğŸ“¡ {name} ìˆ˜ì§‘ ì¤‘...")
            try:
                items = fn(*args)
                all_news += items
                source_map[name] = len(items)
                st.write(f"  âœ… {name}: {len(items)}ê±´")
            except Exception as e:
                source_map[name] = 0
                st.write(f"  âš ï¸ {name}: {e}")

        all_news = dedup(all_news)
        all_news.sort(key=lambda x: x.get("published_at", ""), reverse=True)
        st.session_state[f"{prefix}news_data"] = all_news
        st.session_state[f"{prefix}source_stats"] = source_map
        st.session_state[f"{prefix}summary_quick"] = ""
        st.session_state[f"{prefix}summary_deep"] = ""
        st.session_state[f"{prefix}provider"] = ""

        if use_ai and all_news:
            if ai_provider == "Gemini 2.5 Pro" and GEMINI_API_KEY:
                st.write("ğŸ¤– Gemini 2.5 Proë¡œ ë¶„ì„ ìƒì„± ì¤‘...")
                q, d = summarize_gemini(all_news, GEMINI_API_KEY, prompt_quick, prompt_deep)
                st.session_state[f"{prefix}summary_quick"] = q
                st.session_state[f"{prefix}summary_deep"] = d
                st.session_state[f"{prefix}provider"] = "Gemini 2.5 Pro"
            elif ai_provider == "GPT-4o-mini" and OPENAI_API_KEY:
                st.write("ğŸ¤– GPT-4o-minië¡œ ë¶„ì„ ìƒì„± ì¤‘...")
                q, d = summarize_openai(all_news, OPENAI_API_KEY, prompt_quick, prompt_deep)
                st.session_state[f"{prefix}summary_quick"] = q
                st.session_state[f"{prefix}summary_deep"] = d
                st.session_state[f"{prefix}provider"] = "GPT-4o-mini"
            else:
                st.write("âš ï¸ AI API í‚¤ê°€ ì—†ì–´ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        status.update(label=f"âœ… ìˆ˜ì§‘ ì™„ë£Œ â€” ì´ {len(all_news)}ê±´ (ì¤‘ë³µ ì œê±° í›„)", state="complete")


# â”€â”€ í˜„ì¬ ëª¨ë“œ ë°ì´í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
prefix = "stock_" if is_stock else "coin_"
news_data = st.session_state[f"{prefix}news_data"]
source_stats = st.session_state[f"{prefix}source_stats"]
summary_quick = st.session_state[f"{prefix}summary_quick"]
summary_deep = st.session_state[f"{prefix}summary_deep"]
provider = st.session_state[f"{prefix}provider"]

# â”€â”€ í—¤ë” (ëª¨ë“œë³„) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if is_stock:
    st.markdown(f"""
    <div class="main-header">
      <h1>ğŸ“ˆ ë¯¸êµ­ ì£¼ì‹ ë§ˆì¼“ ë¦¬í¬íŠ¸</h1>
      <div class="sub">ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ ì£¼ìš” ë‰´ìŠ¤ í†µí•© ìˆ˜ì§‘ Â· AI ìš”ì•½ &nbsp;|&nbsp; {TODAY_STR} (KST)</div>
    </div>
    """, unsafe_allow_html=True)
else:
    st.markdown(f"""
    <div class="main-header">
      <h1>ğŸª™ ì½”ì¸ ë‰´ìŠ¤ ì¢…í•© ë¦¬í¬íŠ¸</h1>
      <div class="sub">ë©€í‹° ì†ŒìŠ¤ í†µí•© ìˆ˜ì§‘ + AI ìš”ì•½ | {TODAY_STR} (KST)</div>
    </div>
    """, unsafe_allow_html=True)


# â”€â”€ ê²°ê³¼ í‘œì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not news_data:
    if is_stock:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ **ì£¼ì‹ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ **ì½”ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    st.stop()

# ì†ŒìŠ¤ë³„ í†µê³„
st.markdown('<div class="sec-title">ğŸ“Š ì†ŒìŠ¤ë³„ ìˆ˜ì§‘ í˜„í™©</div>', unsafe_allow_html=True)
total_col, *src_cols = st.columns([1] + [1] * min(len(source_stats), 6))
with total_col:
    accent = "#64ffda" if is_stock else "#f7931a"
    st.markdown(f"""
    <div style="background:#161b22;border:1px solid #21262d;border-top:3px solid {accent};
                border-radius:10px;padding:14px 10px;text-align:center">
      <div style="font-size:1.5rem;font-weight:700;color:{accent}">{len(news_data)}</div>
      <div style="font-size:.75rem;color:#8b949e;margin-top:4px">ì´ ë‰´ìŠ¤</div>
    </div>""", unsafe_allow_html=True)
for col, (src, cnt) in zip(src_cols, list(source_stats.items())[:6]):
    color = src_color(src)
    with col:
        st.markdown(f"""
        <div style="background:#161b22;border:1px solid #21262d;border-top:3px solid {color};
                    border-radius:10px;padding:14px 10px;text-align:center">
          <div style="font-size:1.5rem;font-weight:700;color:{color}">{cnt}</div>
          <div style="font-size:.72rem;color:#8b949e;margin-top:4px;word-break:break-all">{src}</div>
        </div>""", unsafe_allow_html=True)

# AI ìš”ì•½
if summary_quick or summary_deep:
    provider_label = f" <span style='font-size:.8rem;color:#8b949e'>by {provider}</span>" if provider else ""
    st.markdown(f'<div class="sec-title">ğŸ¤– AI ë¶„ì„{provider_label}</div>', unsafe_allow_html=True)
    tab_quick, tab_deep = st.tabs(["âš¡ Quick Summary", "ğŸ”¬ Deep Dive"])
    with tab_quick:
        st.markdown(summary_quick or "_ìš”ì•½ ì—†ìŒ_")
    with tab_deep:
        st.markdown(summary_deep or "_ë¶„ì„ ì—†ìŒ_")

# ë‰´ìŠ¤ ëª©ë¡
st.markdown(f'<div class="sec-title">ğŸ“‹ ì „ì²´ ë‰´ìŠ¤ ëª©ë¡ ({len(news_data)}ê±´)</div>', unsafe_allow_html=True)
col_search, col_src = st.columns([3, 1])
with col_search:
    search_q = st.text_input(
        "ğŸ” ê²€ìƒ‰",
        placeholder="í‹°ì»¤/ê¸°ì—…ëª… ë˜ëŠ” ì½”ì¸/í‚¤ì›Œë“œ ì…ë ¥...",
        label_visibility="collapsed",
    )
with col_src:
    all_sources = sorted(set(item["source"] for item in news_data))
    filter_src = st.selectbox("ì†ŒìŠ¤ í•„í„°", ["ì „ì²´"] + all_sources, label_visibility="collapsed")

filtered = news_data
if search_q:
    q = search_q.lower()
    filtered = [n for n in filtered if q in n["title"].lower() or q in (n.get("description") or "").lower()]
if filter_src != "ì „ì²´":
    filtered = [n for n in filtered if n["source"] == filter_src]

st.caption(f"{len(filtered)}ê±´ í‘œì‹œ ì¤‘")
for i, item in enumerate(filtered, 1):
    render_news_card(item, i)

# í‘¸í„°
sources_stock = "Finnhub Â· Yahoo Finance Â· CNBC Â· MarketWatch Â· MNI Markets Â· MKT News"
sources_coin = "CryptoPanic Â· CoinDesk Â· cryptonews.net Â· coincarp Â· The Block Â· cryptonews.com Â· Decrypt"
footer_src = sources_stock if is_stock else sources_coin
st.markdown(f"""
<div style="text-align:center;padding:24px 16px;color:#6e7681;font-size:.8rem; border-top:1px solid #21262d;margin-top:32px">
  ë°ì´í„° ì¶œì²˜: {footer_src}
  &nbsp;|&nbsp; ìƒì„±: {NOW_KST.strftime('%Y-%m-%d %H:%M')} KST
</div>
""", unsafe_allow_html=True)
